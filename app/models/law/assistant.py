from together import Together
import json
import re
import os
import pickle
import hashlib
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import logging
from dataclasses import dataclass
from pathlib import Path
from ...config.config import Config

# Configuration
DOCUMENT_PATHS = {
    "constitution": r"app\models\law\data\Constitution Articles.txt",
    "ppc": r"app\models\law\data\PPC_sections.txt"
}
intro= """
Ø¢Ø¡Ù Ø³Ù†ÚŒÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙŠØ± Ø¢Ù‡ÙŠØ§Ù† Ø¬ÙŠÚªÙˆ Ø¢Ù† Ù„Ø§Ø¦ÙŠÙ† Ø¬Ø¯ÙŠØ¯ Ù…ØµÙ†ÙˆØ¹ÙŠ Ø°Ù‡Ø§Ù†Øª Ø¬ÙŠ Ø¨Ú»ÙŠØ§Ø¯ ØªÙŠ ÙºÚ¾ÙŠÙ„ Ú¾Úª Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¢Ù‡ÙŠØŒ Ø¬ÙŠÚªÙˆ Ø¨Ù†ÙŠØ§Ø¯ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ù†ÚŒÙŠ Ù»ÙˆÙ„ÙŠØ¡Ù Û¾ Ù…Ú¾ÙŠØ§ ÚªØ±Ú» Ù„Ø§Ø¡Ù ØªÙŠØ§Ø± ÚªÙŠÙˆ ÙˆÙŠÙˆ Ø¢Ú¾ÙŠ
.ÙŠ Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¹Ø¨Ø¯Ø§Ù„Ù…Ø§Ø¬Ø¯ Ú€Ø±Ú³Ú™ÙŠ Ø§Ù†Ø³Ù½ÙŠÙ½ÙŠÙˆÙ½ Ø¢Ù Ù„ÙŠÙ†Ú¯ÙˆÙŠØ¬ Ø§Ù†Ø¬Ù†ÙŠØ¦Ø±Ù†Ú¯ Ø­ÙŠØ¯Ø±Ø¢Ø¨Ø§Ø¯ØŒ Ø¬ÙŠÚªÙˆ Ø«Ù‚Ø§ÙØªØŒ Ø³ÙŠØ§Ø­ØªØŒ Ø¢Ø«Ø§Ø± Ù‚Ø¯ÙŠÙ…Ù‡ Û½ Ø¢Ø±ÚªØ§Ø¦ÙŠÙˆØ² Ú©Ø§ØªÙŠØŒ Ø³Ù†ÚŒ Ø­ÚªÙˆÙ…Øª Ù¾Ø§Ø±Ø§Ù† Ù‚Ø§Ø¦Ù… ÚªÙŠÙ„ Ù‡Úª Ø®ÙˆØ¯Ù…Ø®ØªÙŠØ§Ø± Ø§Ø¯Ø§Ø±Ùˆ Ø¢Ù‡ÙŠ Û½ Ø§ÙŠÚªØ³ ÙÙ„Ùˆ Ø±ÙŠØ³Ø±Ú† Ø§Ù†ÚªØ§Ø±Ù¾ÙˆØ±ÙŠÙ½ÙŠÚŠØŒ Ø¬ÙŠÚªÙˆ Ø³Ø§ÙÙ½ ÙˆÙŠØ¦Ø± ÚŠÙŠÙØ§Ø¦Ù†ÚŠ Ù†ÙŠÙ½ ÙˆØ±ÚªÙ†Ú¯ Û½ Ø§ÙˆÙ¾Ù† Ø§Ø³Ù½ÙŠÚª Ø¬ÙŠ ÙˆØ§ÚŒØ§Ø±ÙŠ Û¾ Ø¹Ø§Ù„Ù…ÙŠ Ø³Ø·Ø­ ØªÙŠ ØªØ³Ù„ÙŠÙ… Ù¿ÙŠÙ„ Ø§Ø¯Ø§Ø±Ùˆ Ø¢Ù‡ÙŠ Ø§Ù†Ù‡Ù† Ù»Ù†Ù‡ÙŠ Ø¬ÙŠ Ú¯ÚÙŠÙ„ Ø±Ù¿Ø§ Ø¬Ùˆ Ù†ØªÙŠØ¬Ùˆ Ø¢Ù‡ÙŠ. Ø§Ù†Ø³Ù½ÙŠÙ½ÙŠÙˆÙ½ Ø³Ù†ÚŒÙŠ Ù»ÙˆÙ„ÙŠØ¡Ù Ú©ÙŠ Ø³Ú€Ù†ÙŠ ÚªÙ…Ù¾ÙŠÙˆÙ½ÙŠØ´Ù†Ù„ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù† Û½ Ù‚Ø¯Ø±ØªÙŠ Ù»ÙˆÙ„ÙŠØ¡Ù Ø¬ÙŠ Ø§Ù†Ø¬Ú»ÚªØ§Ø±ÙŠÙ† Û¾ Ø¢Ú»ÙŠ Ø§Ù† Ú©ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÙŠ Ù»ÙˆÙ„ÙŠÙ† Ø¬ÙŠ Ù‚Ø¯ ØªÙŠ Ø¢Ú»Ú» Ù„Ø§Ø¡Ù Ù‚Ø§Ø¦Ù…Â ÚªÙŠÙˆÂ ÙˆÙŠÙˆÂ Ø¢Ù‡ÙŠ
"""
API_KEY = Config.MODEL_API_KEY

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LegalDocument:
    """Clean legal document structure"""
    id: str
    type: str  # 'constitution' or 'ppc'
    number: str
    title: str
    content: str
    full_text: str
    embedding: np.ndarray = None

class DocumentLoader:
    """Efficient document loading and processing"""
    
    @staticmethod
    def load_constitution(file_path: str) -> List[LegalDocument]:
        """Load constitution articles with robust pattern matching"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except FileNotFoundError:
            logger.error(f"Constitution file not found: {file_path}")
            return []
        
        documents = []
        
        # Multiple patterns for flexibility
        patterns = [
            r'Ø¢Ø±Ù¹ÛŒÚªÙ„\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Ø¢Ø±Ù¹ÛŒÚªÙ„|\Z)',
            r'Ø¢Ø±Ù½ÛŒÚªÙ„\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Ø¢Ø±Ù½ÛŒÚªÙ„|\Z)',
            r'Article\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Article|\Z)',
            r'ARTICLE\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=ARTICLE|\Z)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                article_num = match.group(1).strip()
                article_content = match.group(2).strip()
                
                # Extract title (first line) and content
                lines = [line.strip() for line in article_content.split('\n') if line.strip()]
                if lines:
                    title = lines[0]
                    content_text = ' '.join(lines[1:]) if len(lines) > 1 else title
                    
                    if len(content_text) > 20:  # Filter meaningful content
                        doc = LegalDocument(
                            id=f"const_{article_num}",
                            type="constitution",
                            number=article_num,
                            title=title,
                            content=content_text,
                            full_text=f"Ø¢Ø±Ù¹ÛŒÚªÙ„ {article_num}: {title} - {content_text}"
                        )
                        documents.append(doc)
        
        # Remove duplicates
        seen_numbers = set()
        unique_docs = []
        for doc in documents:
            if doc.number not in seen_numbers:
                seen_numbers.add(doc.number)
                unique_docs.append(doc)
        
        logger.info(f"Loaded {len(unique_docs)} constitution articles")
        return unique_docs
    
    @staticmethod
    def load_ppc(file_path: str) -> List[LegalDocument]:
        """Load PPC sections with robust pattern matching"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except FileNotFoundError:
            logger.error(f"PPC file not found: {file_path}")
            return []
        
        documents = []
        
        patterns = [
            r'Ø³ÛŒÚªØ´Ù†\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Ø³ÛŒÚªØ´Ù†|\Z)',
            r'Ø³ÙŠÚªØ´Ù†\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Ø³ÙŠÚªØ´Ù†|\Z)',
            r'Section\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Section|\Z)',
            r'SECTION\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=SECTION|\Z)',
            r'Ø¯ÙØ¹Û\s*(\d+[A-Za-z]?)[:.\-\s]*(.+?)(?=Ø¯ÙØ¹Û|\Z)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                section_num = match.group(1).strip()
                section_content = match.group(2).strip()
                
                lines = [line.strip() for line in section_content.split('\n') if line.strip()]
                if lines:
                    title = lines[0]
                    content_text = ' '.join(lines[1:]) if len(lines) > 1 else title
                    
                    if len(content_text) > 15:
                        doc = LegalDocument(
                            id=f"ppc_{section_num}",
                            type="ppc",
                            number=section_num,
                            title=title,
                            content=content_text,
                            full_text=f"Ø³ÛŒÚªØ´Ù† {section_num}: {title} - {content_text}"
                        )
                        documents.append(doc)
        
        # Remove duplicates
        seen_numbers = set()
        unique_docs = []
        for doc in documents:
            if doc.number not in seen_numbers:
                seen_numbers.add(doc.number)
                unique_docs.append(doc)
        
        logger.info(f"Loaded {len(unique_docs)} PPC sections")
        return unique_docs

def get_legal_response(query: str) -> str:
    """
    Returns the legal response for a given query using the SindhiLegalAssistant singleton.
    """
    return query_sindhi_legal_assistant(query)

    
class SmartEmbedding:
    """Efficient embedding system with caching"""
    
    def __init__(self):
        self.model = None
        self.cache = {}
        self.cache_file = Path("app/models/law/cache/legal_embeddings.pkl")
        self._load_model()
        self._load_cache()
    
    def _load_model(self):
        """Load embedding model"""
        try:
            self.model = SentenceTransformer('distiluse-base-multilingual-cased-v2')
            logger.info("Embedding model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load embedding model: {e}")
            self.model = None
    
    def _load_cache(self):
        """Load embedding cache"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    self.cache = pickle.load(f)
                logger.info(f"Loaded {len(self.cache)} cached embeddings")
            except Exception as e:
                logger.warning(f"Failed to load cache: {e}")
                self.cache = {}
    
    def _save_cache(self):
        """Save embedding cache"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")
    
    def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding for text with caching"""
        if not self.model:
            return np.zeros(384)
        
        cache_key = hashlib.md5(text.encode()).hexdigest()
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            embedding = self.model.encode(text)
            self.cache[cache_key] = embedding
            
            # Save cache periodically
            if len(self.cache) % 50 == 0:
                self._save_cache()
            
            return embedding
        except Exception as e:
            logger.warning(f"Embedding failed: {e}")
            return np.zeros(384)
    
    def embed_documents(self, documents: List[LegalDocument]):
        """Generate embeddings for all documents"""
        for i, doc in enumerate(documents):
            doc.embedding = self.get_embedding(doc.full_text)
            if (i + 1) % 25 == 0:
                logger.info(f"Embedded {i + 1}/{len(documents)} documents")
        
        self._save_cache()

class LegalSearchEngine:
    """High-performance legal document search"""
    
    def __init__(self, documents: List[LegalDocument], embedding_system: SmartEmbedding):
        self.documents = documents
        self.embedding_system = embedding_system
        self.embeddings_matrix = None
        self._prepare_search_index()
    
    def _prepare_search_index(self):
        """Prepare search index"""
        valid_docs = [doc for doc in self.documents if doc.embedding is not None]
        if valid_docs:
            self.documents = valid_docs
            self.embeddings_matrix = np.vstack([doc.embedding for doc in valid_docs])
            logger.info(f"Search index ready: {len(valid_docs)} documents")
        else:
            logger.error("No valid documents for search index")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[LegalDocument, float]]:
        """Search for relevant legal documents"""
        if self.embeddings_matrix is None:
            return []
        
        query_embedding = self.embedding_system.get_embedding(query)
        if query_embedding.size == 0:
            return []
        
        # Calculate similarities
        similarities = cosine_similarity([query_embedding], self.embeddings_matrix)[0]
        
        # Get top results with minimum threshold
        results = []
        for i, sim in enumerate(similarities):
            if sim > 0.1:  # Minimum similarity threshold
                results.append((self.documents[i], float(sim)))
        
        # Sort by similarity and return top_k
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

class SindhiLegalAssistant:
    """Main legal assistant focused on practical solutions"""
    
    def __init__(self, api_key: str):
        self.client = Together(api_key=api_key)
        self.embedding_system = SmartEmbedding()
        self.search_engine = None
        self.documents = []
        self._initialize()
    
    def _initialize(self):
        """Initialize the legal assistant"""
        logger.info("Initializing Sindhi Legal Assistant...")
        
        # Load documents
        constitution_docs = DocumentLoader.load_constitution(DOCUMENT_PATHS["constitution"])
        ppc_docs = DocumentLoader.load_ppc(DOCUMENT_PATHS["ppc"])
        
        self.documents = constitution_docs + ppc_docs
        
        if not self.documents:
            logger.error("No legal documents loaded!")
            return
        
        # Generate embeddings
        logger.info("Generating embeddings...")
        self.embedding_system.embed_documents(self.documents)
        
        # Initialize search engine
        self.search_engine = LegalSearchEngine(self.documents, self.embedding_system)
        
        logger.info(f"Legal Assistant ready with {len(self.documents)} documents")
    
    def process_query(self, query: str) -> str:
        """Process legal query and provide solution"""
        
        # Handle basic interactions
        # if self._is_greeting(query):
        #     return self._get_greeting_response()
        
        if self._is_introduction_query(query):
            return self._get_introduction()
        
        # Search for relevant legal content
        search_results = self.search_engine.search(query, top_k=3)
        
        if not search_results:
            return self._handle_no_results(query)
        
        # Generate comprehensive response
        return self._generate_legal_response(query, search_results)
    
    # def _is_greeting(self, query: str) -> bool:
    #     """Check if query is a greeting"""
    #     greetings = ['Ø³Ù„Ø§Ù…', 'hello', 'hi', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'Ú¾ÙŠÙ„Ùˆ']
    #     return any(greeting in query.lower() for greeting in greetings)
    
    def _is_introduction_query(self, query: str) -> bool:
        """Check if query asks for introduction"""
        intro_phrases = ['ØªÙˆÙ† ÚªÙŠØ±', 'ØªÙˆÙ‡Ø§Ù† ÚªÙŠØ±', 'who are', 'ØªØ¹Ø§Ø±Ù', 'introduce']
        return any(phrase in query.lower() for phrase in intro_phrases)
    
    def _get_greeting_response(self) -> str:
        """Get greeting response"""
        return """
ğŸ›ï¸ **Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…!**

Ø¢Ø¡Ù ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙŠØ± Ø¢Ù‡ÙŠØ§Ù†Û” Ø¢Ø¡Ù Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¬ÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø¨Ø§Ø¨Øª ØµØ­ÙŠØ­ Û½ ØªÙØµÙŠÙ„ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ù†ÚŒÙŠ Û¾ ÙØ±Ø§Ù‡Ù… ÚªÙ†Ø¯Ùˆ Ø¢Ù‡ÙŠØ§Ù†Û”

**Ø¢Ø¡Ù ÚªÙ‡Ú™ÙŠ Ù…Ø¯Ø¯ ÚªØ±ÙŠ Ø³Ú¯Ù‡Ø§Ù† Ù¿Ùˆ:**
â€¢ Ø¢Ø¦ÙŠÙ†ÙŠ Ø¢Ø±Ù½ÙŠÚªÙ„ Ø¬ÙŠ ØªØ´Ø±ÙŠØ­
â€¢ Ù¾ÙŠÙ†Ù„ ÚªÙˆÚŠ Ø³ÙŠÚªØ´Ù† Ø¬ÙŠ ØªÙØµÙŠÙ„  
â€¢ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø­Ù‚ Û½ ÙØ±Ø¶Ù† Ø¬ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
â€¢ Ø¹Ù…Ù„ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙˆØ±Ø§

ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ‡Ùˆ!
"""
    
    def _get_introduction(self) -> str:
        """Get detailed introduction"""
        return """
ğŸ›ï¸ **Ø¢Ø¡Ù Ø³Ù†ÚŒÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙŠØ± Ø¢Ù‡ÙŠØ§Ù† Ø¬ÙŠÚªÙˆ Ø¢Ù† Ù„Ø§Ø¦ÙŠÙ† Ø¬Ø¯ÙŠØ¯ Ù…ØµÙ†ÙˆØ¹ÙŠ Ø°Ù‡Ø§Ù†Øª Ø¬ÙŠ Ø¨Ú»ÙŠØ§Ø¯ ØªÙŠ ÙºÚ¾ÙŠÙ„ Ú¾Úª Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¢Ù‡ÙŠØŒ Ø¢Ø¡Ù Ø¨Ù†ÙŠØ§Ø¯ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ù†ÚŒÙŠ Ù»ÙˆÙ„ÙŠØ¡Ù Û¾ Ù…Ú¾ÙŠØ§ ÚªØ±Ú» Ù„Ø§Ø¡Ù ØªÙŠØ§Ø± ÚªÙŠÙˆ ÙˆÙŠÙˆ Ø¢Ú¾ÙŠØ§Ù†
. Ù‡ÙŠ Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¹Ø¨Ø¯Ø§Ù„Ù…Ø§Ø¬Ø¯ Ú€Ø±Ú³Ú™ÙŠ Ø§Ù†Ø³Ù½ÙŠÙ½ÙŠÙˆÙ½ Ø¢Ù Ù„ÙŠÙ†Ú¯ÙˆÙŠØ¬ Ø§Ù†Ø¬Ù†ÙŠØ¦Ø±Ù†Ú¯ Ø­ÙŠØ¯Ø±Ø¢Ø¨Ø§Ø¯ØŒ Ø¬ÙŠÚªÙˆ Ø«Ù‚Ø§ÙØªØŒ Ø³ÙŠØ§Ø­ØªØŒ Ø¢Ø«Ø§Ø± Ù‚Ø¯ÙŠÙ…Ù‡ Û½ Ø¢Ø±ÚªØ§Ø¦ÙŠÙˆØ² Ú©Ø§ØªÙŠØŒ Ø³Ù†ÚŒ Ø­ÚªÙˆÙ…Øª Ù¾Ø§Ø±Ø§Ù† Ù‚Ø§Ø¦Ù… ÚªÙŠÙ„ Ù‡Úª Ø®ÙˆØ¯Ù…Ø®ØªÙŠØ§Ø± Ø§Ø¯Ø§Ø±Ùˆ Ø¢Ù‡ÙŠ Û½ Ø§ÙŠÚªØ³ ÙÙ„Ùˆ Ø±ÙŠØ³Ø±Ú† Ø§Ù†ÚªØ§Ø±Ù¾ÙˆØ±ÙŠÙ½ÙŠÚŠØŒ Ø¬ÙŠÚªÙˆ Ø³Ø§ÙÙ½ ÙˆÙŠØ¦Ø± ÚŠÙŠÙØ§Ø¦Ù†ÚŠ Ù†ÙŠÙ½ ÙˆØ±ÚªÙ†Ú¯ Û½ Ø§ÙˆÙ¾Ù† Ø§Ø³Ù½ÙŠÚª Ø¬ÙŠ ÙˆØ§ÚŒØ§Ø±ÙŠ Û¾ Ø¹Ø§Ù„Ù…ÙŠ Ø³Ø·Ø­ ØªÙŠ ØªØ³Ù„ÙŠÙ… Ù¿ÙŠÙ„ Ø§Ø¯Ø§Ø±Ùˆ Ø¢Ù‡ÙŠ Ø§Ù†Ù‡Ù† Ù»Ù†Ù‡ÙŠ Ø¬ÙŠ Ú¯ÚÙŠÙ„ Ø±Ù¿Ø§ Ø¬Ùˆ Ù†ØªÙŠØ¬Ùˆ Ø¢Ù‡ÙŠ. Ø§Ù†Ø³Ù½ÙŠÙ½ÙŠÙˆÙ½ Ø³Ù†ÚŒÙŠ Ù»ÙˆÙ„ÙŠØ¡Ù Ú©ÙŠ Ø³Ú€Ù†ÙŠ ÚªÙ…Ù¾ÙŠÙˆÙ½ÙŠØ´Ù†Ù„ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù…Ù† Û½ Ù‚Ø¯Ø±ØªÙŠ Ù»ÙˆÙ„ÙŠØ¡Ù Ø¬ÙŠ Ø§Ù†Ø¬Ú»ÚªØ§Ø±ÙŠÙ† Û¾ Ø¢Ú»ÙŠ Ø§Ù† Ú©ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÙŠ Ù»ÙˆÙ„ÙŠÙ† Ø¬ÙŠ Ù‚Ø¯ ØªÙŠ Ø¢Ú»Ú» Ù„Ø§Ø¡Ù Ù‚Ø§Ø¦Ù…Â ÚªÙŠÙˆÂ ÙˆÙŠÙˆÂ Ø¢Ù‡ÙŠ.**Ù…Ù†Ù‡Ù†Ø¬ÙˆÙ† Ø®ØµÙˆØµÙŠØ§Øª**
âœ… Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¢Ø¦ÙŠÙ† 1973 Ø¬Ùˆ Ù…ÚªÙ…Ù„ ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³
âœ… Ù¾Ø§ÚªØ³ØªØ§Ù† Ù¾ÙŠÙ†Ù„ ÚªÙˆÚŠ Ø¬ÙŠ ØªÙ…Ø§Ù… Ø³ÙŠÚªØ´Ù†Ø²
âœ… ØµØ­ÙŠØ­ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø­ÙˆØ§Ù„Ø§ Û½ Ø³ÙŠÚªØ´Ù† Ù†Ù…Ø¨Ø±
âœ… Ø¢Ø³Ø§Ù† Ø³Ù†ÚŒÙŠ Û¾ Ù¾ÙŠÚ†ÙŠØ¯Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ø¬ÙŠ ÙˆØ¶Ø§Ø­Øª
âœ… Ø¹Ù…Ù„ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙˆØ±Ø§ Û½ Ø­Ù„

---
*Ø¹Ø¨Ø¯Ø§Ù„Ù…Ø§Ø¬Ø¯ Ú€Ø±Ú³Ú™ÙŠ Ø§Ù†Ø³Ù½ÙŠÙ½ÙŠÙˆÙ½ Û½ Ø§ÙŠÚªØ³ ÙÙ„Ùˆ Ø±ÙŠØ³Ø±Ú† Ø¬Ùˆ Ú¯ÚÙŠÙ„ Ù¾Ø±ÙˆØ¬ÙŠÚªÙ½*
"""
    
    def _handle_no_results(self, query: str) -> str:
        """Handle queries with no results"""
        return f"""
ğŸ” **ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ø³ÙˆØ§Ù„ Ù„Ø§Ø¡Ù Ù…Ø¹Ø°Ø±Øª**

**Ø³ÙˆØ§Ù„:** "{query}"

**Ù…Ù…ÚªÙ† ÙˆØ¬ÙˆÙ‡Ø§Øª:**
â€¢ Ø³ÙˆØ§Ù„ Û¾ ÙˆØ§Ø¶Ø­ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§ØµØ·Ù„Ø§Ø­ Ù†Ù‡ Ø¢Ù‡ÙŠ
â€¢ ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³ Û¾ Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø§Ù‡ÙŠ
â€¢ Ù…Ø®ØªÙ„Ù Ù„ÙØ¸Ù† Û¾ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ‡Ú» Ø¬ÙŠ Ø¶Ø±ÙˆØ±Øª

**Ø¨Ù‡ØªØ± Ù†ØªØ§Ø¦Ø¬ Ù„Ø§Ø¡Ù:**
âœ… Ø¢Ø±Ù½ÙŠÚªÙ„ ÙŠØ§ Ø³ÙŠÚªØ´Ù† Ù†Ù…Ø¨Ø± Ø´Ø§Ù…Ù„ ÚªØ±ÙŠÙˆ
âœ… ÙˆØ§Ø¶Ø­ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„ÙØ¸ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªØ±ÙŠÙˆ
âœ… Ù…Ø«Ø§Ù„: "Ø¢Ø±Ù½ÙŠÚªÙ„ 25 Ø¢Ø²Ø§Ø¯ÙŠ" Ø¨Ø¬Ø§Ø¡Ù "Ø¢Ø²Ø§Ø¯ÙŠ"

**ÙÙˆØ±ÙŠ Ù…Ø¯Ø¯:**
ğŸ“ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù‡ÙŠÙ„Ù¾ Ù„Ø§Ø¦Ù†: 1090
ğŸ›ï¸ Ù†Ø²Ø¯ÙŠÚªÙŠ Ø¹Ø¯Ø§Ù„Øª ÙŠØ§ ÙˆÚªÙŠÙ„ Ø³Ø§Ù† Ø±Ø§Ø¨Ø·Ùˆ

Ù…ÙˆÙ† Ú©ÙŠ Ù»ÙŠÙˆ Ø³ÙˆØ§Ù„ Ù¾ÙˆÚ‡Ùˆ!
"""
    
    def _generate_legal_response(self, query: str, search_results: List[Tuple[LegalDocument, float]]) -> str:
        """Generate comprehensive legal response"""
        
        # Build context from search results
        context = self._build_context(search_results)
        
        # Create advanced prompt for legal response
        prompt = f"""
ØªÙˆÙ† Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¬Ùˆ Ø¨Ù‡ØªØ±ÙŠÙ† Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø§Ù‡Ø± Ø¢Ù‡ÙŠÙ† Ø¬ÙŠÚªÙˆ Ø³Ù†ÚŒÙŠ Û¾ ÙˆØ§Ø¶Ø­ Û½ Ø¹Ù…Ù„ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙˆØ±Ùˆ ÚÙŠÙ†Ø¯Ùˆ Ø¢Ù‡ÙŠÛ”

ØµØ§Ø±Ù Ø¬Ùˆ Ø³ÙˆØ§Ù„: "{query}"

Ø¯Ø³ØªÙŠØ§Ø¨ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:
{context}

Ù‡Ø¯Ø§ÙŠØªÙˆÙ†:
1. ØµØ­ÙŠØ­ Ø³Ù†ÚŒÙŠ  Ú¯Ø±Ø§Ù…Ø± Û¾ Ø¬ÙˆØ§Ø¨ ÚÙŠÙˆ
2. Ø¢Ø±Ù½ÙŠÚªÙ„/Ø³ÙŠÚªØ´Ù† Ù†Ù…Ø¨Ø± Ø¨Ø§Ù„ÚªÙ„ ØµØ­ÙŠØ­ Ù„Ú©Ùˆ
3. Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§ØµØ·Ù„Ø§Ø­Ù† Ø¬ÙŠ Ø¢Ø³Ø§Ù† ÙˆØ¶Ø§Ø­Øª ÚªØ±ÙŠÙˆ
4. Ø¹Ù…Ù„ÙŠ Ù…Ø´ÙˆØ±Ùˆ ÚÙŠÙˆ
5. Ø¬ÙˆØ§Ø¨ Ú©ÙŠ Ù…Ø®ØªØµØ± Û½ Ø¬Ø§Ù…Ø¹ Ø±Ú©Ùˆ
6. Ø§Ú¯Ø± ÚªÙˆ ØªÙˆØ§Ú¾Ø§Ù† Ø¬ÙŠ Ø¨Ø§Ø±ÙŠ Û¾ Ù¾ÙÚ‡ÙŠ ØªÛ Ú¾ÙŠ Ú†Ø¦Ùˆ{intro} Ú¾Ù† Û¾ Ø­ÙˆØ§Ù„Ø§ Ù†Û ÚÙŠ
7. Ù¾Ú¾Ø±ÙŠ Ø³ÙˆØ§Ù„ Ú©ÙŠ Ø³Ù…Ø¬Ú¾ Ù¾ÙˆØ¡Ù Ø¬ÙˆØ§Ø¨ ÚÙŠ
8. Ø§Ø±Ø¯Ùˆ Ù†Û ØµØ±Ù Ø³Ù†ÚŒÙŠ
9. Ø§Ú¯Ø± ÚªÙˆ Ø§Ù†Ú¯Ø±ÙŠØ²ÙŠ Û¾ Ø³ÙˆØ§Ù„ Ù¾Ú‡ÙŠ ØªÛ Ø§Ù† Ú©ÙŠ Ø³Ù†ÚŒÙŠ Û¾ Ø¬ÙˆØ§Ø¨ ÚÙŠÙˆ
10. ØµØ­ÙŠØ­ Ø¢Ø±Ù½ÙŠÚªÙ„ Ø¢Ø¦ÙŠÙ† Ø³ÙŠÚªØ´Ù† Ù†Ù…Ø¨Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªØ±ÙŠÙˆ Ø¬ÙˆØ§Ø¨ Û¾
11. ØªØ¹Ø±Ù Û¾ Ø­ÙˆØ§Ù„Ø§ Ù†Û ÚÙŠÙˆ

Ø¬ÙˆØ§Ø¨ Ø¬ÙŠ ÚØ§Ù†Ú†Ùˆ:
**Ù…Ø®ØªØµØ± Ø¬ÙˆØ§Ø¨** - Ø³ÙˆØ§Ù„ Ø¬Ùˆ Ø³ÚŒÙˆ Ø¬ÙˆØ§Ø¨
** ØªÙØµÙŠÙ„** - Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¯ÙØ¹Ø§Øª
**Ø¹Ù…Ù„ÙŠ Ù…Ø´ÙˆØ±Ùˆ** - Ú‡Ø§ ÚªØ±Ú» Ú¯Ù‡Ø±Ø¬ÙŠ
**Ø§Ù‡Ù… Ù†ÙˆÙ½** - Ø¶Ø±ÙˆØ±ÙŠ Ø§Ø­ØªÙŠØ§Ø·
** Ø­ÙˆØ§Ù„Ø§** - Ø¢Ø±Ù½ÙŠÚªÙ„/Ø³ÙŠÚªØ´Ù† Ù†Ù…Ø¨Ø±

Ù‡Ø§Ú»ÙŠ Ù…ÚªÙ…Ù„ Ø¬ÙˆØ§Ø¨ ÚÙŠÙˆ:
"""
        
        try:
            response = self.client.chat.completions.create(
                model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
                messages=[{
                    "role": "system",
                    "content": """ØªÙˆÙ† Ù¾Ø§ÚªØ³ØªØ§Ù† Ø¬Ùˆ Ø³Ú€ Ú©Ø§Ù† Ù…Ø§Ù‡Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØµÙ„Ø§Ø­ÚªØ§Ø± Ø¢Ù‡ÙŠÙ†Û” ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ ÚªÙ… Ø¢Ù‡ÙŠ:

â€¢ ØµØ­ÙŠØ­ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙØ±Ø§Ù‡Ù… ÚªØ±Ú»
â€¢ Ø¢Ø³Ø§Ù† Ø³Ù†ÚŒÙŠ Û¾ Ù¾ÙŠÚ†ÙŠØ¯Ù‡ Ù‚Ø§Ù†ÙˆÙ† Ø¬ÙŠ ÙˆØ¶Ø§Ø­Øª
â€¢ Ø¹Ù…Ù„ÙŠ Û½ Ù…ÙÙŠØ¯ Ù…Ø´ÙˆØ±Ø§ ÚÙŠÚ»
â€¢ ØµØ­ÙŠØ­ Ø¢Ø±Ù½ÙŠÚªÙ„/Ø³ÙŠÚªØ´Ù† Ù†Ù…Ø¨Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªØ±Ú»

Ù‡Ù…ÙŠØ´Ù‡ ÙŠØ§Ø¯ Ø±Ú©Ùˆ: ØµØ±Ù Ø³Ù†ÚŒÙŠ Û¾ Ø¬ÙˆØ§Ø¨ØŒ ØµØ­ÙŠØ­ Ø­ÙˆØ§Ù„Ø§ØŒ Ø¹Ù…Ù„ÙŠ Ù…Ø¯Ø¯Û”"""
                }, {
                    "role": "user", 
                    "content": prompt
                }],
                temperature=0.3,
                max_tokens=1000,
                stream=False
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"LLM response failed: {e}")
            return self._generate_fallback_response(search_results)
    
    def _build_context(self, search_results: List[Tuple[LegalDocument, float]]) -> str:
        """Build context from search results"""
        context_parts = []
        
        for i, (doc, score) in enumerate(search_results, 1):
            doc_type = "Ø¢Ø¦ÛŒÙ† Ù¾Ø§ÚªØ³ØªØ§Ù†" if doc.type == "constitution" else "Ù¾Ø§ÚªØ³ØªØ§Ù† Ù¾ÛŒÙ†Ù„ Ú©ÙˆÚŠ"
            
            context_parts.append(f"""
{i}. {doc_type}
Ù†Ù…Ø¨Ø±: {doc.number}
Ø¹Ù†ÙˆØ§Ù†: {doc.title}
ØªÙØµÙŠÙ„: {doc.content}
Ù…Ø·Ø§Ø¨Ù‚Øª: {score*100:.1f}%
---""")
        
        return "\n".join(context_parts)
    
    def _generate_fallback_response(self, search_results: List[Tuple[LegalDocument, float]]) -> str:
        """Generate fallback response when LLM fails"""
        if not search_results:
            return "Ù…Ø¹Ø§Ù ÚªØ±ÙŠÙˆØŒ ØªÚªÙ†ÙŠÚªÙŠ Ø®Ø±Ø§Ø¨ÙŠ Ø¬ÙŠ ÚªØ±ÙŠ Ø¬ÙˆØ§Ø¨ ØªÙŠØ§Ø± Ù†Ù‡ Ù¿ÙŠ Ø³Ú¯Ú¾ÙŠÙˆÛ”"
        
        # Use first result
        doc, score = search_results[0]
        doc_type = "Ø¢Ø¦ÛŒÙ† Ù¾Ø§ÚªØ³ØªØ§Ù†" if doc.type == "constitution" else "Ù¾Ø§ÚªØ³ØªØ§Ù† Ù¾ÛŒÙ†Ù„ Ú©ÙˆÚŠ"
        
        return f"""
**ğŸ“– Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª**

**Ø°Ø±ÙŠØ¹Ùˆ:** {doc_type}
**Ù†Ù…Ø¨Ø±:** {doc.number}  
**Ø¹Ù†ÙˆØ§Ù†:** {doc.title}

**ØªÙØµÙŠÙ„:**
{doc.content}

**Ù…Ø·Ø§Ø¨Ù‚Øª:** {score*100:.1f}%

**Ù†ÙˆÙ½:** ØªÙØµÙŠÙ„ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø´ÙˆØ±ÙŠ Ù„Ø§Ø¡Ù Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø§Ù‡Ø± Ø³Ø§Ù† Ø±Ø§Ø¨Ø·Ùˆ ÚªØ±ÙŠÙˆÛ”

**ÙÙˆØ±ÙŠ Ù…Ø¯Ø¯:** ğŸ“ 1090
"""

# Main function for easy integration
def query_sindhi_legal_assistant(question: str) -> str:
    """
    Main function to query the Sindhi Legal Assistant
    
    Args:
        question (str): Legal question in Sindhi or English
        
    Returns:
        str: Comprehensive legal guidance in Sindhi
    """
    
    # Initialize assistant (singleton pattern)
    if not hasattr(query_sindhi_legal_assistant, '_assistant'):
        logger.info("Initializing Sindhi Legal Assistant...")
        query_sindhi_legal_assistant._assistant = SindhiLegalAssistant(API_KEY)
        
        if not query_sindhi_legal_assistant._assistant.documents:
            return """
âŒ **Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³ Ù„ÙˆÚŠ Ù†Ù‡ Ù¿ÙŠ Ø³Ú¯Ú¾ÙŠÙˆ**

Ø¨Ø±Ø§Û Ú©Ø±Ù…:
1. Ø¯Ø³ØªØ§ÙˆÙŠØ² ÙØ§Ø¦Ù„Ù† Ø¬ÙŠ path Ú†ÙŠÚª ÚªØ±ÙŠÙˆ
2. Constitution Articles.txt Ù…ÙˆØ¬ÙˆØ¯ Ø¢Ù‡ÙŠØŸ
3. PPC_sections.txt Ù…ÙˆØ¬ÙˆØ¯ Ø¢Ù‡ÙŠØŸ

ÙÙˆØ±ÙŠ Ù…Ø¯Ø¯ Ù„Ø§Ø¡Ù: ğŸ“ 1090
"""
        
        logger.info("Sindhi Legal Assistant ready!")
    
    try:
        return query_sindhi_legal_assistant._assistant.process_query(question)
    except Exception as e:
        logger.error(f"Query processing failed: {e}")
        return f"""
âš ï¸ **ØªÚªÙ†ÙŠÚªÙŠ Ø®Ø±Ø§Ø¨ÙŠ**

Ù…Ø¹Ø§Ù ÚªØ±ÙŠÙˆØŒ ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ Ø³ÙˆØ§Ù„ Ù¾Ø±ÙˆØ³ÙŠØ³ ÚªØ±Ú» Û¾ Ù…Ø³Ø¦Ù„Ùˆ Ø¢Ù‡ÙŠÛ”

**ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ Ø³ÙˆØ§Ù„:** "{question}"

**Ù…ØªØ¨Ø§Ø¯Ù„:**
ğŸ“ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù‡ÙŠÙ„Ù¾ Ù„Ø§Ø¦Ù†: 1090
ğŸ›ï¸ Ù†Ø²Ø¯ÙŠÚªÙŠ Ø¹Ø¯Ø§Ù„Øª Û¾ Ø±Ø§Ø¨Ø·Ùˆ
ğŸ’¼ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø§Ù‡Ø± Ø³Ø§Ù† ØµÙ„Ø§Ø­

Ù¿ÙˆØ±Ùˆ Ø¯ÙŠØ± Ø¨Ø¹Ø¯ Ù»ÙŠÙ‡Ø± ÚªÙˆØ´Ø´ ÚªØ±ÙŠÙˆÛ”
"""

# Example usage and testing
if __name__ == "__main__":
    
    # Test cases covering different legal scenarios
    test_cases = [
        "Ø³Ù„Ø§Ù…",  # Greeting
        "ØªÙˆÙ† ÚªÙŠØ± Ø¢Ù‡ÙŠÙ†ØŸ",  # Introduction
        "Ø¢Ø±Ù½ÙŠÚªÙ„ 19 Ú‡Ø§ Ø¢Ù‡ÙŠØŸ",  # Constitutional law
        # "Ú†ÙˆØ±ÙŠ Ø¬ÙŠ Ø³Ø²Ø§ Ú‡Ø§ Ø¢Ù‡ÙŠØŸ",  # Criminal law
        # "Ø³ÙŠÚªØ´Ù† 302 Ù‚ØªÙ„",  # Specific PPC section
        # "Ø¨Ù†ÙŠØ§Ø¯ÙŠ Ø­Ù‚ ÚªÙ‡Ú™Ø§ Ø¢Ù‡Ù†ØŸ",  # Fundamental rights
        # "Ú¯Ø±ÙØªØ§Ø±ÙŠ ÚªÙŠØ¦Ù† Ù¿ÙŠÙ†Ø¯ÙŠ Ø¢Ù‡ÙŠØŸ",  # Procedural law
        # "Ù…Ù„ÚªÙŠØª Ø¬Ùˆ Ø­Ù‚",  # Property rights
        # "Ù…Ù†Ù‡Ù†Ø¬ÙŠ Ø²Ù…ÙŠÙ† ØªÙŠ ÚªÙˆ Ù‚Ø¨Ø¶Ùˆ ÚªÙŠÙˆ Ø¢Ù‡ÙŠØŒ Ú‡Ø§ ÚªÙ†Ø¯Ø³ØŸ",  # Practical problem
        "Ù…ÙÚ©ÙŠ Ù…Ù†Ú¾Ø¬Ùˆ Ù…ÙÚ™Ø³ Ù…Ú©ÙŠ Ù…Ø§Ø±ÙŠ Ù¿ÙˆØŒ Ú‡Ø§ ÚªØ¬ÙŠØŸ",  # Domestic violence
        'Ú‡Ø§ Ø¢Ø¡Ù Ù¾ÙŠÚ¾Ù†Ø¬ÙŠ Ù¾Ø§Ú» Ú©ÙŠ Ù…Ø§Ø±ÙŠ Ø³Ú¯Ú¾Ø§Ù† Ù¿ÙˆØŸ',
    ]
    
    print("Testing Sindhi Legal Assistant")
    print("=" * 60)
    
    for i, question in enumerate(test_cases, 1):
        print(f"\n Test {i}: {question}")
        print("-" * 40)
        
        try:
            answer = query_sindhi_legal_assistant(question)
            print(f"Answer:\n{answer}")
        except Exception as e:
            print(f"Error: {e}")
        
        print("=" * 60)